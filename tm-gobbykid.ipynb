{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we analyze our corpus through **topic modelling**, a text analysis method that allows us to identify the topics within the texts. The main tool we have used is the [**Little MALLET Wrapper**](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php). Mallet is a software, originally developed in Java, that topic models texts using **[Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)** (**LDA**), a generative probabilistic model able to discover topics (i.e. set of words that, taken together, suggest a shared theme), and then automatically classify any individual document within the collection in terms of how \"relevant\" it is to each of the discovered topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Working in this Jupyter notebook requires having Java Development Kit and MALLET pre-installed. For set up instructions, we followed [Melanie Walsh's tutorial](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Java and MALLET are installed, we need to setup Little MALLET Wrapper and import some libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the path to the Java-based MALLET software\n",
    "path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
    "\n",
    "#install the Little MALLET Wrapper and the data visualization library Seaborn\n",
    "'''\n",
    "!pip install little_mallet_wrapper\n",
    "!pip install seaborn\n",
    "'''\n",
    "\n",
    "#import the above packages and libraries for working with files and the file system\n",
    "import little_mallet_wrapper\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Data From Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we topic model the Gobby Kid project corpus, we need to process the text files and prepare them for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign to the \"directory\" variable the path to the directory containing our documents\n",
    "m_directory = \"C:/Users/media/Desktop/gobbykid/balanced_corpus/m\"\n",
    "f_directory = \"C:/Users/media/Desktop/gobbykid/balanced_corpus/f\"\n",
    "\n",
    "#use `glob.gob()` function to make a list of all the `.txt` files in that directory.\n",
    "m_files = glob.glob(f\"{m_directory}/*.txt\")\n",
    "f_files = glob.glob(f\"{f_directory}/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#print(f\"Male corpus: \\n\", m_files)\n",
    "#print(f\"Female corpus: \\n\", f_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Texts and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we process our texts with the function `little_mallet_wrapper.process_string()`, which takes every individual text file, transform all the text to lowercase, removes stopwords, punctuation, and numbers. We add the processed text to our master list `training_data`, i.e. the data we will use to train our LDA topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "The default list of stop words in `little_mallet_wrapper` is quite limited. We want to extend it in order to get **more meaningful words** in our topics, i.e. we want to leave out of our model not only grammatical words, but also extremely common words like \"said\" or \"think\". Moreover, we expect characters' names to be highly weighting in the training phase, but not particularly relevant to get an overview of the common themes in the dataset: we want to exclude from the results also the characters' names we have previously extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import LMW stop words list (stored in the variable \"STOPS\"), in order to extend it\n",
    "from little_mallet_wrapper.little_mallet_wrapper import STOPS as lmw_stopwords #!important: import from little_mallet_wrapper.little_mallet_wrapper, otherwise it will look for STOPS inside __init__.py (inside little_mallet_wrapper) and it will not find it!\n",
    "\n",
    "#print(\"Default stop words in lmw: \", lmw_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most common words\n",
    "First, let's calculate the most common words in our corpus. We will tokenize the texts at word level and join all their words in one list; then we'll extract the **frequency distribution** of each word in the dataset using `nltk`.\n",
    "Finally, we can examine manually if, amongst the most common ones, there is any word we don't want to be included in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "dataset_words = [] #initialize a list to contain all the words in the whole dataset\n",
    "for file in tqdm(m_files + f_files): #tqdm just displays a progress bar\n",
    "    file_text = open(file, encoding='utf-8').read()\n",
    "    file_words = nltk.tokenize.word_tokenize(file_text) #texts are segmented in words\n",
    "    dataset_words.extend(file_words) \n",
    "    \n",
    "allWordDist = nltk.FreqDist(w.lower() for w in dataset_words) #frequency ditribution of each word in the dataset (FreqDist object) \n",
    "\n",
    "k = 400 #set k to the number of words you want retrieved from the frequency distribution\n",
    "most_common_words = [item[0] for item in allWordDist.most_common(k)] #create a list with all the k most common words in the whole corpus. allWordDist.most_common() returns a list of tuples of the form: (\"word\", frequency)\n",
    "\n",
    "#print(f\"These are the {k} most frequent words: \", most_common_words)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a manual analysis of the most common words, we decide to add some of them to the stop words list, particularly the ones carrying \"less meaning\", such as words representing *basic* actions which might be very common in the narration (such as \"went\" or \"came\"), or related to the presence of dialogues (e.g. \"said\", \"called\", \"asked\"...). \n",
    "\n",
    "Then, we add to the words to ignore the characters' names we [previously extracted](#link alla sezione della NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\" style=\"color:red; font-weight:bold\">\n",
    "Qui sotto importo il file in cui è salvata una LISTA contenente tutti i nomi COMPLETI dei personaggi (entrambi i corpora insieme). Questi nomi vengono poi processati più sotto da `get_characters_names`, che li divide e crea una nuova lista, `characters_names`, che contiene i nomi completi ma anche le singole parti (es. solo nome o solo cognome). <u>Se possibile, questa parte sarebbe meglio modificarla, alla fine,</u> per fare in modo che non si ripetano passaggi che il codice di Tommaso eventualmente fa già,  o in ogni caso per non ricorrere ad altri file esterni al jupyter notebook unico. Cioè quando avremo messo tutto quanto su un unico jupyter notebook, è il caso che modifichiamo la parte qui sotto e introduciamo i seguenti passaggi:\n",
    "* richiamiamo direttamente la variabile in cui è salvato l'output delle funzioni che ha fatto Tommaso per estrarre le NER, e la trasformiamo in una *unica* lista con tutti i nomi dei personaggi, da entrambi i corpora indivisi\n",
    "* passiamo questa lista in input a `get_characters_names`, che divide le stringhe\n",
    "* aggiungiamo la lista output di `get_characters_names` alla lista di stopwords per il pre-processing\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the list is formatted as a list in a .txt file you can use the code below\n",
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "with open('C:/Users/media/Desktop/gobbykid/tm/all_characters_file.txt', 'rb') as f:  #change the path to the location where the result of run.py is stored\n",
    "    retrieved_characters = pickle.load(f) #retrieves the variable from the specified file, in which all the characters' FULL names are stored in a list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "csv_path = \"C:/Users/media/Desktop/gobbykid/gobbykid-characters-extraction/characters_extraction_and_gender_recognition/characters.csv\"\n",
    "\n",
    "retrieved_characters =[]\n",
    "with open(csv_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "                retrieved_characters.extend(ast.literal_eval(row[\"male_characters_names\"]) + ast.literal_eval(row[\"female_characters_names\"]) + ast.literal_eval(row['unknown_gender_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters_names (characters_list):\n",
    "    # empty honorifics list if you don't want to include in the model character-related words\n",
    "    honorifics = []\n",
    "    #honorifics = [\"mr\",\"master\",\"mrs\",\"miss\",\"lady\",\"sir\",\"dame\",\"lord\",\"sister\",\"mother\",\"aunt\",\"uncle\",\"doctor\",\"captain\",\"father\",\"count\",\"professor\",\"major\",\"little\",\"big\",\"old\",\"king\",\"don\",\"dr\",\"queen\",\"boy\",\"girl\",\"green\",\"black\",\"grey\",\"gray\",\"golden\",\"silver\",\"blue\", \"long\"]\n",
    "    output_set = set()\n",
    "    #add to the previous list also the first names and last names taken by themselves (eg both \"john smith\" and \"john\" get in the list), excluding all the words in \"honorifics\"\n",
    "    \n",
    "\n",
    "    for item in characters_list:\n",
    "        if \" \" in item:\n",
    "            minilist = item.split()\n",
    "            for w in minilist:\n",
    "                if w not in honorifics:\n",
    "                    output_set.add(w)\n",
    "        else:\n",
    "            output_set.add(item)\n",
    "    \n",
    "    output_list = sorted(output_set) #convert the set to a list\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = lmw_stopwords #initialize final list of custom stop words as containing the default stop words of lmw\n",
    "\n",
    "more_stopwords = [\"one\",\"two\",\"ones\",\"say\",\"says\",\"said\",\"think\",\"thinks\",\"thought\",\"thing\",\"things\",\"go\",\"goes\",\"went\",\"come\",\"comes\",\"came\",\"coming\",\"much\",\"get\",\"gets\",\"got\",\"ask\",\"asks\",\"asked\",\"didn\",\"would\",\"could\",\"three\",\"other\",\"another\",\"until\",\"till\",\"upon\",\"shall\",\"make\",\"made\",\"might\",\"must\",\"going\",\"way\", \"thou\", \"thee\", \"seems\", \"seem\", \"seemed\", \"never\", \"tell\", \"told\", \"tells\", \"wouldn\", \"tha\", \"like\", \"however\",\"let\",\"rather\",\"yes\", \"no\",\"little\", \"old\", \"well\", \"always\", \"never\", \"time\", \"long\", \"see\", \"saw\", \"sees\"]\n",
    "\n",
    "# list of gender-related words to remove\n",
    "gendered_stopwords = ['guy','dr','spokesman','chairman',\"men's\",'men','boy',\n",
    "'boys','brother','brothers','dad','dads','dude','father',\n",
    "'fathers','gentleman','gentlemen','god','grandfather','grandpa',\n",
    "'grandson','groom','he','himself','his','husband','pastor','husbands','king','male','man',\n",
    "'mr','nephew','nephews','priest','prince','son','sons','uncle','uncles',\n",
    "'waiter','widower','widowers','lord','master',\"mrs\",\"miss\",\"lady\",\"sir\",\"dame\",\"lord\",\"sister\",\"mother\",\"aunt\",\"uncle\",\"doctor\",\"father\",\"count\",\"professor\", 'heroine','drss','spokeswoman','chairwoman',\"women's\",'actress','women',\"she's\",'her','aunt','aunts','bride','daughter','daughters','female','fiancee','girl',\n",
    "'girls','goddess','granddaughter','grandma','grandmother',\n",
    "'herself','ladies','lady','mom','moms','mother','mothers','mrs','ms','niece',\n",
    "'nieces','priestess','princess','queens','sister','sisters','waitress',\n",
    "'widow','widows','wife','wives','woman','lady','mistress','queen']\n",
    "\n",
    "characters_names = get_characters_names(retrieved_characters) #the function returns a list of strings that are (part of) a character's name\n",
    "\n",
    "custom_stopwords.extend(more_stopwords)\n",
    "custom_stopwords.extend(characters_names)\n",
    "#removes words like \"mr\", \"mrs\", \"mother\", \"father\", etc.\n",
    "custom_stopwords.extend(gendered_stopwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process\n",
    "Now that the custom stop words list is ready, we can finally pre-process our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "#initialize training_data as empty\n",
    "from encodings.utf_8 import decode\n",
    "\n",
    "\n",
    "m_training_data = []\n",
    "f_training_data = []\n",
    "\n",
    "#process each file and add it to the training_data list\n",
    "for file in tqdm(m_files):\n",
    "    text = open(file, encoding='utf-8').read() \n",
    "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove', stop_words=custom_stopwords, remove_stop_words=True, stop_words_extra=custom_stopwords)\n",
    "    m_training_data.append(processed_text)\n",
    "\n",
    "for file in tqdm(f_files):\n",
    "    text = open(file, encoding='utf-8').read()\n",
    "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove', stop_words=custom_stopwords, remove_stop_words=True, stop_words_extra=custom_stopwords)\n",
    "    f_training_data.append(processed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that processing the texts is a time-consuming operation and we might want to train our model several times in order to chose the best parameters, we can optionally store the output of the text processing in files of choice, so as to be able to retrieve the ready-to-use texts directly, i.e. skipping the processing phase, the next time we use this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stores processed texts in specific files\n",
    "\n",
    "#MALE CORPUS\n",
    "m_pointer = 0\n",
    "for processed_book in m_training_data:\n",
    "    booktitle= Path(m_files[m_pointer]).stem\n",
    "    with open(f\"m_preprocess/Processed_{booktitle}.txt\", 'w') as f:\n",
    "        f.write(processed_book)\n",
    "        m_pointer+=1\n",
    "\n",
    "\n",
    "#FEMALE CORPUS\n",
    "f_pointer = 0\n",
    "for processed_book in f_training_data:\n",
    "    booktitle= Path(f_files[f_pointer]).stem\n",
    "    with open(f\"f_preprocess/Processed_{booktitle}.txt\", 'w') as f:\n",
    "        f.write(processed_book)\n",
    "        f_pointer+=1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#re-create the training data variable from saved files\n",
    "\n",
    "#MALE CORPUS\n",
    "m_training_data = []\n",
    "m_processed_files_paths = glob.glob(\"m_preprocess/*.txt\") #creates a list with the file paths of the processed texts\n",
    "for processed_text in m_processed_files_paths:\n",
    "    ready_text = open(processed_text).read()\n",
    "    m_training_data.append(ready_text)\n",
    "\n",
    "#FEMALE CORPUS\n",
    "f_training_data = []\n",
    "f_processed_files_paths = glob.glob(\"f_preprocess/*.txt\") #creates a list with the file paths of the processed texts\n",
    "for processed_text in f_processed_files_paths:\n",
    "    ready_text = open(processed_text).read()\n",
    "    f_training_data.append(ready_text)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also making a master list of the original text of the novels for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_original_texts = []\n",
    "for file in m_files:\n",
    "    text = open(file, encoding='utf-8').read()\n",
    "    m_original_texts.append(text)\n",
    "\n",
    "f_original_texts = []\n",
    "for file in f_files:\n",
    "    text = open(file, encoding='utf-8').read()\n",
    "    f_original_texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [`Path().stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem) function to extract just the last part of the file path without the \".txt\" file extension, and save the file name (formatted as *year of publication* + *author's last name* + *book's title*) as title for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_book_titles = [Path(file).stem for file in m_files]\n",
    "f_book_titles = [Path(file).stem for file in f_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#MALE CORPUS\n",
    "m_book_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEMALE CORPUS\n",
    "f_book_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Data Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get training data summary statistics by using the function ```little_mallet_wrapper.print_dataset_stats()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"----Male writers corpus statistics:\")\n",
    "little_mallet_wrapper.print_dataset_stats(m_training_data)\n",
    "\n",
    "print(f\"\\n----Female writers corpus statistics:\")\n",
    "little_mallet_wrapper.print_dataset_stats(f_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train the model with the function [`quick_train_topic_model(path_to_mallet, output_directory_path, num_topics, training_data)`](https://github.com/maria-antoniak/little-mallet-wrapper#quick_train_topic_modelpath_to_mallet-output_directory_path-num_topics-training_data). First of all, then, we need to set the variables the function is taking as parameters.\n",
    "The `path_to_mallet` and `training_data` variables have been already specified, we only need to create the `num_topics` and the `output_directory_path` variables. The latter specifies where Little MALLET Wrapper will find and output all the results of our topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the number of topics we want returned\n",
    "num_topics = 7\n",
    "\n",
    "#set desired output directory/ies (will be created inside current directory)\n",
    "m_output_directory_path = 'topic-model-output/m_results'#MALE CORPUS\n",
    "f_output_directory_path = 'topic-model-output/f_results'#FEMALE CORPUS\n",
    "\n",
    "\n",
    "#set topic model output files! No need to change anything below here\n",
    "#MALE CORPUS\n",
    "Path(f\"{m_output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "m_path_to_training_data           = f\"{m_output_directory_path}/training.txt\"\n",
    "m_path_to_formatted_training_data = f\"{m_output_directory_path}/mallet.training\"\n",
    "m_path_to_model                   = f\"{m_output_directory_path}/mallet.model.{str(num_topics)}\"\n",
    "m_path_to_topic_keys              = f\"{m_output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
    "m_path_to_topic_distributions     = f\"{m_output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
    "\n",
    "#FEMALE CORPUS\n",
    "Path(f\"{f_output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "f_path_to_training_data           = f\"{f_output_directory_path}/training.txt\"\n",
    "f_path_to_formatted_training_data = f\"{f_output_directory_path}/mallet.training\"\n",
    "f_path_to_model                   = f\"{f_output_directory_path}/mallet.model.{str(num_topics)}\"\n",
    "f_path_to_topic_keys              = f\"{f_output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
    "f_path_to_topic_distributions     = f\"{f_output_directory_path}/mallet.topic_distributions.{str(num_topics)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're going to train our topic model with `little_mallet_wrapper.quick_train_topic_model()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "#MALE CORPUS\n",
    "little_mallet_wrapper.quick_train_topic_model(path_to_mallet,\n",
    "                                             m_output_directory_path,\n",
    "                                             num_topics,\n",
    "                                             m_training_data)\n",
    "\n",
    "#FEMALE CORPUS\n",
    "little_mallet_wrapper.quick_train_topic_model(path_to_mallet,\n",
    "                                             f_output_directory_path,\n",
    "                                             num_topics,\n",
    "                                             f_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Topics and Top Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the `num_topics` topics that the topic model extracted from our corpus we use the [`little_mallet_wrapper.load_topic_keys()`](https://github.com/maria-antoniak/little-mallet-wrapper#load_topic_keystopic_keys_path) function, reading and processing the output from the file in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MALE CORPUS\n",
    "m_topics = little_mallet_wrapper.load_topic_keys(m_path_to_topic_keys) #list of lists of strings\n",
    "print(f\"\\n------------ ♂️ MALE CORPUS ------------\\n\")\n",
    "for topic_number, topic in enumerate(m_topics):\n",
    "    print(f\"Topic: {topic_number}\\n\\n{topic}\\n\")\n",
    "\n",
    "#FEMALE CORPUS\n",
    "f_topics = little_mallet_wrapper.load_topic_keys(f_path_to_topic_keys) #list of lists of strings\n",
    "print(f\"\\n\\n------------ ♀️ FEMALE CORPUS ------------\\n\")\n",
    "for topic_number, topic in enumerate(f_topics):\n",
    "    print(f\"Topic: {topic_number}\\n\\n{topic}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Topic Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MALLET also calculates the probability distribution of these topics over every single document in the corpus, that is, the probability that each topic exists in a specified document. Therefore, we can use these probability distributions to examine how strongly the extracted topics are associated with a given document (book) in our corpus.\n",
    "\n",
    "To get the topic distributions, we're going to use the [`little_mallet_wrapper.load_topic_distributions()`](https://github.com/maria-antoniak/little-mallet-wrapper#load_topic_distributionstopic_distributions_path) function, which will read and process the MALLET topic model output (contained in the file whose path is stored in the `path_to_topic_distributions` variable) and return a list of the probability distributions of each topic over each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the probability distributions (list of lists) in a variable\n",
    "m_topic_distributions = little_mallet_wrapper.load_topic_distributions(m_path_to_topic_distributions) #list of lists of integers\n",
    "f_topic_distributions = little_mallet_wrapper.load_topic_distributions(f_path_to_topic_distributions) #list of lists of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can analyze the probability distribution of the topics extracted from a specific dataset over one specific book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "#specify the title/label of the desidered document in a variable\n",
    "book_to_check = \"1882_delaramee-bimbi\"\n",
    "#get document's index in the list and store it in a variable\n",
    "book_number = f_book_titles.index(book_to_check)\n",
    "#specify the set of topics - i.e. extracted from male/female corpus - (and relative topic distributions) for which you want to get the probability distribution over the input book \n",
    "topics_lol = f_topics\n",
    "topic_distribution_lol = f_topic_distributions\n",
    "\n",
    "#calculate the probability distribution of the female/male corpus topics over the input book\n",
    "print(f\"Topic Distributions for {f_book_titles[book_number]}\\n\")\n",
    "for topic_number, (topic, topic_distribution) in enumerate(zip(topics_lol, topic_distribution_lol[book_number])):\n",
    "    print(f\"Topic {topic_number} {topic} \\nProbability: {round(topic_distribution, 3)}\\n\")\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Heatmap of Topics and Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize and compare these topic probability distributions with a heatmap by using the [`little_mallet_wrapper.plot_categories_by_topics_heatmap()`](https://github.com/maria-antoniak/little-mallet-wrapper#plot_categories_by_topics_heatmaplabels-topic_distributions-topic_keys-output_pathnone-target_labelsnone-dimnone) function.\n",
    "\n",
    "First, we only need to specify the `target_labels` parameter, telling the function which documents we'd like to be plotted in the graph. **Below, we are plotting *some* documents in the *female* corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_target_labels = ['1841_martineau-the-settlers-at-home',\n",
    " '1857_browne-grannys-wonderful-chair',\n",
    " '1857_tucker-the-rambles-of-a-rat',\n",
    " '1862_ewing-melchiors-dream-and-other-tales',\n",
    " '1869_ewing-mrs-overtheways-remembrances',\n",
    " '1869_ewing-the-land-of-lost-toys',\n",
    " '1870_ewing-the-brownies-and-other-tales',\n",
    " '1872_craik-the-adventure-of-a-brownie',\n",
    " '1872_de-la-ramee-a-dog-of-flanders'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEMALE CORPUS\n",
    "little_mallet_wrapper.plot_categories_by_topics_heatmap(f_book_titles,\n",
    "                                      f_topic_distributions,\n",
    "                                      f_topics, \n",
    "                                      f_output_directory_path + '/categories_by_topics.pdf',\n",
    "                                      target_labels=f_target_labels,\n",
    "                                      dim= (13, 9)\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker squares in this heatmap represent a high probability for the corresponding topic (compared to everyone else in the heatmap) and the lighter squares in the heatmap represent a low probability for the corresponding topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are plotting the data relative to *some* documents from the *male* corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MALE CORPUS\n",
    "\n",
    "m_target_labels = ['1848_marryat-the-little-savage',\n",
    " '1851_ruskin-the-king-of-the-golden-river',\n",
    " '1857_ballantyne-the-coral-island-a-tale-of-the-pacific-ocean',\n",
    " '1857_hughes-tom-browns-school-days',\n",
    " '1862_farrar-st-winfreds-the-world-of-school',\n",
    " '1863_kingsley-the-water-babies',\n",
    " '1865_carroll-alices-adventures-in-wonderland',\n",
    " '1869_dickens-david-copperfield'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MALE CORPUS\n",
    "little_mallet_wrapper.plot_categories_by_topics_heatmap(m_book_titles,\n",
    "                                      m_topic_distributions,\n",
    "                                      m_topics, \n",
    "                                      m_output_directory_path + '/categories_by_topics.pdf',\n",
    "                                      target_labels=m_target_labels,\n",
    "                                      dim= (13, 9)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Top Titles Per Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the books that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
    "\n",
    "To display only the title of the book, we'll first need to make two dictionaries for each corpus, which will allow us to find the corresponding book title and the original text from a given training document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MALE CORPUS\n",
    "m_training_data_book_titles = dict(zip(m_training_data, m_book_titles))\n",
    "m_training_data_original_text = dict(zip(m_training_data, m_original_texts))\n",
    "\n",
    "#FEMALE CORPUS\n",
    "f_training_data_book_titles = dict(zip(f_training_data, f_book_titles))\n",
    "f_training_data_original_text = dict(zip(f_training_data, f_original_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll use the function `display_top_titles_per_topic()` that will display the top text titles for a given topic, in the selected corpus (male/female). This function accepts a given `topic_number`, a desired `number_of_documents` to display, and the indication of the corpus we chose to analyse (set `corpus_gender` to either `\"male\"` or `\"female\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_top_titles_per_topic(topic_number=0, number_of_documents=5, corpus_gender=str()):\n",
    "    corpus_gender = corpus_gender.lower()\n",
    "    if corpus_gender == \"male\":\n",
    "        print(f\"------------ ♂️ MALE CORPUS ------------\\nTopic {topic_number}\\n\\n{m_topics[topic_number]}\\n\")\n",
    "\n",
    "        for probability, document in little_mallet_wrapper.get_top_docs(m_training_data, m_topic_distributions, topic_number, n=number_of_documents):\n",
    "            print(round(probability, 4), m_training_data_book_titles[document] + \"\\n\")\n",
    "        return\n",
    "    elif corpus_gender == \"female\":\n",
    "        print(f\"------------ ♀️ FEMALE CORPUS ------------\\nTopic {topic_number}\\n\\n{f_topics[topic_number]}\\n\")\n",
    "\n",
    "        for probability, document in little_mallet_wrapper.get_top_docs(f_training_data, f_topic_distributions, topic_number, n=number_of_documents):\n",
    "            print(round(probability, 4), f_training_data_book_titles[document] + \"\\n\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=0, number_of_documents=5, corpus_gender=\"male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=0, number_of_documents=5, corpus_gender=\"female\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of most relevant topics\n",
    "\n",
    "After having analyzed the output topics, we try to identify those ones that seem to carry most interesting information for our analysis. \n",
    "\n",
    "From this standpoint, we identify the following topics as being the ones that carry potentially gender-related information:\n",
    "\n",
    "<p align=\"center\" style=\"color:red; font-weight:bold\">\n",
    "QUI SCEGLIAMO DEI TOPICS CHE VOGLIAMO ANALIZZARE E LI RENDIAMO ESPLICITI.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold\">For example</span>, to display the top 3 book titles with the highest probability of containing topic 1 in the male corpus and topic 3 in the female corpus, we will run the `display_top_titles_per_topic()` function specifying the desired topics and corpus in input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=1, number_of_documents=3, corpus_gender=\"male\")\n",
    "display_top_titles_per_topic(topic_number=3, number_of_documents=3, corpus_gender=\"female\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the labels for each document in 2 groups for each corpus, pre and post 1800\n",
    "\n",
    "male_pre1800 = [book for book in m_book_titles if book[0:4] <= \"1879\"]\n",
    "male_post1800 = [book for book in m_book_titles if book[0:4] > \"1879\"]\n",
    "female_pre1800 = [book for book in f_book_titles if book[0:4] <= \"1879\"]\n",
    "female_post1800 = [book for book in f_book_titles if book[0:4] > \"1879\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_data_csv(corpus_gender): #creates a csv file, according to the corpus specified in input, to easily retrieve data\n",
    "    corpus_gender.lower\n",
    "\n",
    "    document_labels = [] # list of strings, that are the books' titles\n",
    "    topics = [] #list of lists of strings, length is number of topics\n",
    "    distributions = [] #list of lists of numbers, length is number of documents\n",
    "    \n",
    "    if corpus_gender == \"male\":\n",
    "        topics = m_topics \n",
    "        distributions = m_topic_distributions\n",
    "        document_labels = m_book_titles\n",
    "        csvfile = open('m_tabular_results.csv', 'w', newline='')\n",
    "        \n",
    "    elif corpus_gender == \"female\":\n",
    "        topics = f_topics\n",
    "        distributions = f_topic_distributions\n",
    "        document_labels = f_book_titles\n",
    "        csvfile = open('f_tabular_results.csv', 'w', newline='')\n",
    "\n",
    "    fieldnames = ['book_index','book_title']\n",
    "    for topic_number, topic_words in enumerate(topics):\n",
    "        first_words = topic_words[:4]\n",
    "        field = f\"Topic {topic_number}: {first_words}\"\n",
    "        fieldnames.append(field)\n",
    "    \n",
    "\n",
    "    list_for_csv = []\n",
    "    for row in distributions:\n",
    "        row_to_add =[]\n",
    "        position= distributions.index(row)\n",
    "        row_to_add.append(position)\n",
    "        row_to_add.append(document_labels[position])\n",
    "\n",
    "        \n",
    "        for prob in row:\n",
    "            prob = round(prob, 4)\n",
    "            row_to_add.append(prob)\n",
    "        list_for_csv.append(row_to_add)\n",
    "        \n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    writer.writerow(fieldnames)\n",
    "\n",
    "    for row in list_for_csv:\n",
    "            writer.writerow(row)\n",
    "    csvfile.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_csv(\"male\")\n",
    "create_data_csv(\"female\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
